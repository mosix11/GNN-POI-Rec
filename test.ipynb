{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227178"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "DF_COLUMNS = ['User ID',\n",
    "            'Venue ID',\n",
    "            'Venue Category ID',\n",
    "            'Venue Category Name',\n",
    "            'Latitude',\n",
    "            'Longitude',\n",
    "            'Timezone Offset',\n",
    "            'UTC Time']\n",
    "df = pd.read_csv('data/FNYC/raw_files/dataset_tsmc2014/dataset_TSMC2014_NYC.txt',  sep='\\t', encoding='latin-1', names=DF_COLUMNS)\n",
    "df = df.sort_values(by=['User ID'])\n",
    "df = df.drop_duplicates()\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1306831/2144088096.py:14: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda group: group[['Latitude', 'Longitude']].drop_duplicates().reset_index(drop=True).values.tolist())\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def collect_unique_venue_locations(dataframe):\n",
    "    \"\"\"\n",
    "    Collect all different locations assigned to each unique venue across all records.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): The input dataframe with venue check-in data.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are Venue IDs and values are lists of unique locations (latitude, longitude).\n",
    "    \"\"\"\n",
    "    # Group by 'Venue ID' and aggregate unique locations (Latitude, Longitude)\n",
    "    venue_locations = (\n",
    "        dataframe.groupby('Venue ID', group_keys=False)\n",
    "        .apply(lambda group: group[['Latitude', 'Longitude']].drop_duplicates().reset_index(drop=True).values.tolist())\n",
    "        .to_dict()\n",
    "    )\n",
    "    return venue_locations\n",
    "\n",
    "# Example usage\n",
    "unique_venue_locations = collect_unique_venue_locations(df)\n",
    "\n",
    "# Display venues with multiple locations (if any)\n",
    "venues_with_multiple_locations = {venue: locations for venue, locations in unique_venue_locations.items() if len(locations) > 1}\n",
    "\n",
    "print(len(venues_with_multiple_locations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1306831/2553799605.py:17: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda group: group[['Latitude', 'Longitude']].drop_duplicates().values.tolist())\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "def collect_unique_venue_locations(dataframe):\n",
    "    \"\"\"\n",
    "    Collect all different locations assigned to each unique venue across all records.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): The input dataframe with venue check-in data.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are Venue IDs and values are lists of unique locations (latitude, longitude).\n",
    "    \"\"\"\n",
    "    # Group by 'Venue ID' and aggregate unique locations (Latitude, Longitude)\n",
    "    venue_locations = (\n",
    "        dataframe.groupby('Venue ID')\n",
    "        .apply(lambda group: group[['Latitude', 'Longitude']].drop_duplicates().values.tolist())\n",
    "        .to_dict()\n",
    "    )\n",
    "    return venue_locations\n",
    "\n",
    "def haversine_distance(loc1, loc2):\n",
    "    \"\"\"\n",
    "    Calculate the great-circle distance between two points on the Earth using the Haversine formula.\n",
    "\n",
    "    Args:\n",
    "        loc1 (tuple): (latitude, longitude) of the first location in decimal degrees.\n",
    "        loc2 (tuple): (latitude, longitude) of the second location in decimal degrees.\n",
    "\n",
    "    Returns:\n",
    "        float: Distance between the two points in kilometers.\n",
    "    \"\"\"\n",
    "    # Radius of the Earth in kilometers\n",
    "    R = 6371.0\n",
    "\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1, lon1 = radians(loc1[0]), radians(loc1[1])\n",
    "    lat2, lon2 = radians(loc2[0]), radians(loc2[1])\n",
    "\n",
    "    # Differences in coordinates\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "\n",
    "    # Haversine formula\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    # Distance\n",
    "    return R * c * 1000\n",
    "\n",
    "def calculate_longest_distance(venue_locations):\n",
    "    \"\"\"\n",
    "    Calculate the longest great-circle distance between different locations assigned to each unique venue.\n",
    "\n",
    "    Args:\n",
    "        venue_locations (dict): A dictionary where keys are Venue IDs and values are lists of unique locations (latitude, longitude).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are Venue IDs and values are the longest great-circle distance (if more than one location exists).\n",
    "    \"\"\"\n",
    "    longest_distances = {}\n",
    "\n",
    "    for venue, locations in venue_locations.items():\n",
    "        if len(locations) > 1:\n",
    "            # Calculate all pairwise distances using the Haversine formula\n",
    "            distances = [haversine_distance(loc1, loc2) for loc1, loc2 in combinations(locations, 2)]\n",
    "            longest_distances[venue] = max(distances)\n",
    "        else:\n",
    "            longest_distances[venue] = 0  # No distance if only one location\n",
    "\n",
    "    return longest_distances\n",
    "\n",
    "def get_sorted_venues_by_distance(longest_distances):\n",
    "    \"\"\"\n",
    "    Sort venues by their longest distances in descending order.\n",
    "\n",
    "    Args:\n",
    "        longest_distances (dict): A dictionary where keys are Venue IDs and values are the longest Euclidean distances.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples sorted by longest distance in descending order (Venue ID, Longest Distance).\n",
    "    \"\"\"\n",
    "    return sorted(longest_distances.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Example usage\n",
    "unique_venue_locations = collect_unique_venue_locations(df)\n",
    "longest_distances = calculate_longest_distance(unique_venue_locations)\n",
    "sorted_venues = get_sorted_venues_by_distance(longest_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4ddad40bd22d4dbc8c0d4f91 20875.184303474245\n",
      "4be2144d21d5a59302ca1511 19864.869301420516\n",
      "4f386335e4b08f009a8525de 16384.124820247493\n",
      "4c7170dafa49a1cd60e6a8e3 15937.920618432663\n",
      "4a8c0960f964a520e50c20e3 14393.90774926774\n",
      "4f1708fae4b0044a28cbe14f 11763.24908652051\n",
      "4e0e0b3caeb7a5b33ee5dac1 7748.079960620895\n",
      "4e7b9f93b61c001c6b38f13f 7233.508154016825\n",
      "4e51dcc76284416669b03aec 6345.832450303119\n",
      "4dc7d9d81f6ef43b8a4e609a 5353.036771362292\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for venue, distance in sorted_venues:\n",
    "    print(venue, distance)\n",
    "    i+=1\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded!\n",
      "Dataset already extracted!\n",
      "Dateset statistics before filtering:\n",
      "Number of users: 1083, with min = 100, max = 2693, and avg: 209.76731301939057\n",
      "Number of venues: 38333, with min = 1, max = 1145, and avg: 5.926434142905591\n",
      "Number of venue categories: 400\n",
      "Dateset statistics after filtering:\n",
      "Number of users: 1081, with min = 20, max = 2511, and avg: 136.6096207215541\n",
      "Number of venues: 5128, with min = 10, max = 1145, and avg: 28.797776911076443\n",
      "Number of venue categories: 320\n",
      "Spatial graph sparsity: 0.9934740076810561\n",
      "Temporal graph sparsity: 0.9182665190772997\n",
      "/home/mosix11/Projects/DL/dlpenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name          | Type             | Params | Mode \n",
      "------------------------------------------------------------\n",
      "0  | user_emb      | Embedding        | 553 K  | train\n",
      "1  | poi_emb       | Embedding        | 2.6 M  | train\n",
      "2  | emb_dropout   | Dropout          | 0      | train\n",
      "3  | net           | LSTM             | 8.4 M  | train\n",
      "4  | out_projector | Linear           | 5.3 M  | train\n",
      "5  | loss_fn       | CrossEntropyLoss | 0      | train\n",
      "6  | acc1          | AccuracyK        | 0      | train\n",
      "7  | acc5          | AccuracyK        | 0      | train\n",
      "8  | acc10         | AccuracyK        | 0      | train\n",
      "9  | acc20         | AccuracyK        | 0      | train\n",
      "10 | mrr           | MRR              | 0      | train\n",
      "------------------------------------------------------------\n",
      "16.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "16.8 M    Total params\n",
      "67.336    Total estimated model params size (MB)\n",
      "11        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/mosix11/Projects/DL/dlpenv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (9) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "Epoch 49: 100%|█| 9/9 [00:00<00:00, 19.35it/s, v_num=7, Train/Loss_step=3.310, T\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|██▏                 | 1/9 [00:00<00:00, 15.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|████▍               | 2/9 [00:00<00:00, 10.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████▋             | 3/9 [00:03<00:07,  0.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|████████▉           | 4/9 [00:03<00:04,  1.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|███████████         | 5/9 [00:03<00:02,  1.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|█████████████▎      | 6/9 [00:03<00:01,  1.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|███████████████▌    | 7/9 [00:03<00:01,  1.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|█████████████████▊  | 8/9 [00:03<00:00,  2.05it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████████| 9/9 [00:03<00:00,  2.29it/s]\u001b[A\n",
      "Epoch 99: 100%|█| 9/9 [00:00<00:00, 18.96it/s, v_num=7, Train/Loss_step=3.050, T\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|██▏                 | 1/9 [00:00<00:00, 15.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|████▍               | 2/9 [00:00<00:00,  7.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████▋             | 3/9 [00:00<00:01,  5.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|████████▉           | 4/9 [00:00<00:00,  6.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|███████████         | 5/9 [00:00<00:00,  7.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|█████████████▎      | 6/9 [00:00<00:00,  8.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|███████████████▌    | 7/9 [00:00<00:00,  8.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|█████████████████▊  | 8/9 [00:00<00:00,  9.31it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████████| 9/9 [00:00<00:00, 10.24it/s]\u001b[A\n",
      "Epoch 149: 100%|█| 9/9 [00:00<00:00, 17.43it/s, v_num=7, Train/Loss_step=2.680, \u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|██▏                 | 1/9 [00:00<00:00, 15.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|████▍               | 2/9 [00:00<00:00, 10.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████▋             | 3/9 [00:00<00:00,  6.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|████████▉           | 4/9 [00:00<00:00,  7.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|███████████         | 5/9 [00:00<00:00,  7.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|█████████████▎      | 6/9 [00:00<00:00,  8.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|███████████████▌    | 7/9 [00:00<00:00,  9.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|█████████████████▊  | 8/9 [00:00<00:00,  9.90it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████████| 9/9 [00:00<00:00, 10.87it/s]\u001b[A\n",
      "Epoch 199: 100%|█| 9/9 [00:00<00:00, 20.50it/s, v_num=7, Train/Loss_step=2.430, \u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|██▏                 | 1/9 [00:00<00:00, 15.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|████▍               | 2/9 [00:00<00:00,  9.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████▋             | 3/9 [00:00<00:01,  5.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|████████▉           | 4/9 [00:00<00:00,  6.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|███████████         | 5/9 [00:00<00:00,  7.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|█████████████▎      | 6/9 [00:00<00:00,  8.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|███████████████▌    | 7/9 [00:00<00:00,  8.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|█████████████████▊  | 8/9 [00:00<00:00,  9.46it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████████| 9/9 [00:00<00:00, 10.39it/s]\u001b[A\n",
      "Epoch 249: 100%|█| 9/9 [00:00<00:00, 18.57it/s, v_num=7, Train/Loss_step=2.230, \u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|██▏                 | 1/9 [00:00<00:00, 14.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|████▍               | 2/9 [00:00<00:00,  9.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████▋             | 3/9 [00:00<00:01,  5.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|████████▉           | 4/9 [00:00<00:00,  6.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|███████████         | 5/9 [00:00<00:00,  7.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|█████████████▎      | 6/9 [00:00<00:00,  8.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|███████████████▌    | 7/9 [00:00<00:00,  9.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|█████████████████▊  | 8/9 [00:00<00:00,  9.47it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████████| 9/9 [00:00<00:00, 10.31it/s]\u001b[A\n",
      "Epoch 263:  67%|▋| 6/9 [00:00<00:00, 14.79it/s, v_num=7, Train/Loss_step=2.250, ^CA\n",
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    }
   ],
   "source": [
    "!python train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded!\n",
      "Dataset already extracted!\n",
      "Dateset statistics before filtering:\n",
      "Number of records: 227178\n",
      "Number of users: 1083, with min = 100, max = 2693, and avg: 209.76731301939057\n",
      "Number of venues: 38333, with min = 1, max = 1145, and avg: 5.926434142905591\n",
      "Number of venue categories: 400\n",
      "Dateset statistics after filtering:\n",
      "Number of records: 147675\n",
      "Number of users: 1081, with min = 20, max = 2511, and avg: 136.6096207215541\n",
      "Number of venues: 5128, with min = 10, max = 1145, and avg: 28.797776911076443\n",
      "Number of venue categories: 320\n",
      "1796 19864.869301420516\n",
      "3745 16384.124820247493\n",
      "3469 15937.920618432663\n",
      "3973 14393.90774926774\n",
      "3327 7233.508154016825\n",
      "1 0\n",
      "2 0\n",
      "3 0\n",
      "4 0\n",
      "5 0\n",
      "num venues with more than 200 min check-ins 0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FoursquareNYC\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TrajLSTM\n\u001b[0;32m----> 4\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mFoursquareNYC\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m ds\u001b[38;5;241m.\u001b[39msetup(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m ds\u001b[38;5;241m.\u001b[39msetup(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Projects/DL/files/src/dataset/FoursquareNYC.py:200\u001b[0m, in \u001b[0;36mFoursquareNYC.__init__\u001b[0;34m(self, data_dir, batch_size, num_workers, user_checkin_tsh, venue_checkin_tsh, num_test_checkins, geohash_precision, max_traj_length, traj_sampling_method, temporal_graph_jaccard_mult_set, temporal_graph_jaccard_sim_tsh, spatial_graph_self_loop, spatial_graph_geohash_precision, temporal_graph_self_loop, seed)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_dataset()\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_data()\n\u001b[0;32m--> 200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_form_spatial_graph()\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_form_temporal_graph()\n",
      "File \u001b[0;32m~/Projects/DL/files/src/dataset/FoursquareNYC.py:342\u001b[0m, in \u001b[0;36mFoursquareNYC._preprocess_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    338\u001b[0m         i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum venues with more than 200 min check-ins\u001b[39m\u001b[38;5;124m'\u001b[39m, i)\n\u001b[0;32m--> 342\u001b[0m \u001b[43mexit\u001b[49m()\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSTATS \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_user\u001b[39m\u001b[38;5;124m'\u001b[39m: df_flt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser ID\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique(),\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_pois\u001b[39m\u001b[38;5;124m'\u001b[39m: df_flt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVenue ID\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique(),\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_poi_cat\u001b[39m\u001b[38;5;124m'\u001b[39m: df_flt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVenue Category ID\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique(),\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_time_slots\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m56\u001b[39m\n\u001b[1;32m    351\u001b[0m }\n\u001b[1;32m    353\u001b[0m df_flt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_time(df_flt)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "from src.dataset import FoursquareNYC\n",
    "from src.models import TrajLSTM\n",
    "\n",
    "ds = FoursquareNYC(num_workers=1)\n",
    "ds.setup('fit')\n",
    "ds.setup('test')\n",
    "# model = TrajLSTM(num_user=ds.STATS['num_user'],\n",
    "#                  num_pois=ds.STATS['num_pois'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_user': 1081,\n",
       " 'num_pois': 5128,\n",
       " 'num_poi_cat': 320,\n",
       " 'num_time_slots': 56,\n",
       " 'num_gh_P5': 115,\n",
       " 'num_gh_P6': 1175,\n",
       " 'num_gh_P7': 3548}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.STATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "User ID                                                              1\n",
       "Venue ID             [2, 15, 12, 13, 23, 16, 14, 20, 7, 7, 13, 25, ...\n",
       "Venue Category ID    [2, 14, 12, 10, 20, 15, 13, 17, 7, 7, 10, 6, 1...\n",
       "Geohash P5 ID        [1, 2, 3, 3, 2, 2, 2, 2, 2, 2, 3, 4, 3, 5, 6, ...\n",
       "Geohash P6 ID        [1, 2, 3, 3, 4, 5, 6, 7, 7, 7, 3, 8, 9, 10, 11...\n",
       "Geohash P7 ID        [1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 4, 10, 11, 12, ...\n",
       "Local Time           [2012-04-07 13:42:24, 2012-04-08 14:20:29, 201...\n",
       "Time Slot            [45, 53, 5, 29, 40, 41, 45, 47, 47, 47, 30, 31...\n",
       "Unix Timestamp       [1333806144, 1333894829, 1333974052, 133423676...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.user_train_trajectories.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 63])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dl_train = ds.train_dataloader()\n",
    "dl_test = ds.test_dataloader()\n",
    "x, y, lens = next(iter(dl_train))\n",
    "\n",
    "# model.validation_step(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import torch\n",
    "from IPython.display import display\n",
    "widget = widgets.Output()\n",
    "# with widget:\n",
    "#     display(ds.poi_trajectories)\n",
    "# print(ds.user_train_trajectories.iloc[400])\n",
    "ds.setup(stage='fit')\n",
    "ds.setup(stage='test')\n",
    "dl_train = ds.train_dataloader()\n",
    "dl_test = ds.test_dataloader()\n",
    "x, y, lens = next(iter(dl_train)) \n",
    "user_ids = x[0]\n",
    "pois = x[1]\n",
    "seq_len = pois.size(1)\n",
    "mask = torch.arange(seq_len).expand(len(lens), seq_len) < lens.unsqueeze(1)\n",
    "expanded_user_ids = user_ids.unsqueeze(1).repeat(1, pois.size(1))\n",
    "print(lens)\n",
    "print(expanded_user_ids.shape)\n",
    "print(pois.shape)\n",
    "# print(expanded_user_ids.shape)\n",
    "print(expanded_user_ids * mask)\n",
    "# print(pois.shape)\n",
    "# print(y[1][0])\n",
    "# print(y[1].reshape(-1))\n",
    "# print(lens)\n",
    "# print(x[1].shape)\n",
    "# print(next(iter(dl_test)))\n",
    "# for batch in dl_test:\n",
    "#     pass\n",
    "\n",
    "# display(widget)\n",
    "\n",
    "# lengths1 = np.array(ds.user_train_trajectories['Time Slot'].apply(len))\n",
    "# lengths2 = np.array(ds.user_train_trajectories['Venue ID'].apply(len))\n",
    "# lengths3 = np.array(ds.user_train_trajectories['Geohash ID'].apply(len))\n",
    "\n",
    "# ds.plot_distribution(lengths)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
