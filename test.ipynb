{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227178"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "DF_COLUMNS = ['User ID',\n",
    "            'Venue ID',\n",
    "            'Venue Category ID',\n",
    "            'Venue Category Name',\n",
    "            'Latitude',\n",
    "            'Longitude',\n",
    "            'Timezone Offset',\n",
    "            'UTC Time']\n",
    "df = pd.read_csv('data/FNYC/raw_files/dataset_tsmc2014/dataset_TSMC2014_NYC.txt',  sep='\\t', encoding='latin-1', names=DF_COLUMNS)\n",
    "df = df.sort_values(by=['User ID'])\n",
    "df = df.drop_duplicates()\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1306831/2144088096.py:14: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda group: group[['Latitude', 'Longitude']].drop_duplicates().reset_index(drop=True).values.tolist())\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def collect_unique_venue_locations(dataframe):\n",
    "    \"\"\"\n",
    "    Collect all different locations assigned to each unique venue across all records.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): The input dataframe with venue check-in data.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are Venue IDs and values are lists of unique locations (latitude, longitude).\n",
    "    \"\"\"\n",
    "    # Group by 'Venue ID' and aggregate unique locations (Latitude, Longitude)\n",
    "    venue_locations = (\n",
    "        dataframe.groupby('Venue ID', group_keys=False)\n",
    "        .apply(lambda group: group[['Latitude', 'Longitude']].drop_duplicates().reset_index(drop=True).values.tolist())\n",
    "        .to_dict()\n",
    "    )\n",
    "    return venue_locations\n",
    "\n",
    "# Example usage\n",
    "unique_venue_locations = collect_unique_venue_locations(df)\n",
    "\n",
    "# Display venues with multiple locations (if any)\n",
    "venues_with_multiple_locations = {venue: locations for venue, locations in unique_venue_locations.items() if len(locations) > 1}\n",
    "\n",
    "print(len(venues_with_multiple_locations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1306831/2553799605.py:17: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda group: group[['Latitude', 'Longitude']].drop_duplicates().values.tolist())\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "def collect_unique_venue_locations(dataframe):\n",
    "    \"\"\"\n",
    "    Collect all different locations assigned to each unique venue across all records.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): The input dataframe with venue check-in data.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are Venue IDs and values are lists of unique locations (latitude, longitude).\n",
    "    \"\"\"\n",
    "    # Group by 'Venue ID' and aggregate unique locations (Latitude, Longitude)\n",
    "    venue_locations = (\n",
    "        dataframe.groupby('Venue ID')\n",
    "        .apply(lambda group: group[['Latitude', 'Longitude']].drop_duplicates().values.tolist())\n",
    "        .to_dict()\n",
    "    )\n",
    "    return venue_locations\n",
    "\n",
    "def haversine_distance(loc1, loc2):\n",
    "    \"\"\"\n",
    "    Calculate the great-circle distance between two points on the Earth using the Haversine formula.\n",
    "\n",
    "    Args:\n",
    "        loc1 (tuple): (latitude, longitude) of the first location in decimal degrees.\n",
    "        loc2 (tuple): (latitude, longitude) of the second location in decimal degrees.\n",
    "\n",
    "    Returns:\n",
    "        float: Distance between the two points in kilometers.\n",
    "    \"\"\"\n",
    "    # Radius of the Earth in kilometers\n",
    "    R = 6371.0\n",
    "\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1, lon1 = radians(loc1[0]), radians(loc1[1])\n",
    "    lat2, lon2 = radians(loc2[0]), radians(loc2[1])\n",
    "\n",
    "    # Differences in coordinates\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "\n",
    "    # Haversine formula\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    # Distance\n",
    "    return R * c * 1000\n",
    "\n",
    "def calculate_longest_distance(venue_locations):\n",
    "    \"\"\"\n",
    "    Calculate the longest great-circle distance between different locations assigned to each unique venue.\n",
    "\n",
    "    Args:\n",
    "        venue_locations (dict): A dictionary where keys are Venue IDs and values are lists of unique locations (latitude, longitude).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are Venue IDs and values are the longest great-circle distance (if more than one location exists).\n",
    "    \"\"\"\n",
    "    longest_distances = {}\n",
    "\n",
    "    for venue, locations in venue_locations.items():\n",
    "        if len(locations) > 1:\n",
    "            # Calculate all pairwise distances using the Haversine formula\n",
    "            distances = [haversine_distance(loc1, loc2) for loc1, loc2 in combinations(locations, 2)]\n",
    "            longest_distances[venue] = max(distances)\n",
    "        else:\n",
    "            longest_distances[venue] = 0  # No distance if only one location\n",
    "\n",
    "    return longest_distances\n",
    "\n",
    "def get_sorted_venues_by_distance(longest_distances):\n",
    "    \"\"\"\n",
    "    Sort venues by their longest distances in descending order.\n",
    "\n",
    "    Args:\n",
    "        longest_distances (dict): A dictionary where keys are Venue IDs and values are the longest Euclidean distances.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples sorted by longest distance in descending order (Venue ID, Longest Distance).\n",
    "    \"\"\"\n",
    "    return sorted(longest_distances.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Example usage\n",
    "unique_venue_locations = collect_unique_venue_locations(df)\n",
    "longest_distances = calculate_longest_distance(unique_venue_locations)\n",
    "sorted_venues = get_sorted_venues_by_distance(longest_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4ddad40bd22d4dbc8c0d4f91 20875.184303474245\n",
      "4be2144d21d5a59302ca1511 19864.869301420516\n",
      "4f386335e4b08f009a8525de 16384.124820247493\n",
      "4c7170dafa49a1cd60e6a8e3 15937.920618432663\n",
      "4a8c0960f964a520e50c20e3 14393.90774926774\n",
      "4f1708fae4b0044a28cbe14f 11763.24908652051\n",
      "4e0e0b3caeb7a5b33ee5dac1 7748.079960620895\n",
      "4e7b9f93b61c001c6b38f13f 7233.508154016825\n",
      "4e51dcc76284416669b03aec 6345.832450303119\n",
      "4dc7d9d81f6ef43b8a4e609a 5353.036771362292\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for venue, distance in sorted_venues:\n",
    "    print(venue, distance)\n",
    "    i+=1\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded!\n",
      "Dataset already extracted!\n",
      "Dateset statistics before filtering:\n",
      "Number of users: 1083, with min = 100, max = 2693, and avg: 209.76731301939057\n",
      "Number of venues: 38333, with min = 1, max = 1145, and avg: 5.926434142905591\n",
      "Number of venue categories: 400\n",
      "Dateset statistics after filtering:\n",
      "Number of users: 1081, with min = 20, max = 2511, and avg: 136.6096207215541\n",
      "Number of venues: 5128, with min = 10, max = 1145, and avg: 28.797776911076443\n",
      "Number of venue categories: 320\n",
      "Spatial graph sparsity: 0.9934740076810561\n",
      "Temporal graph sparsity: 0.9182665190772997\n",
      "/home/mosix11/Projects/DL/dlpenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name          | Type             | Params | Mode \n",
      "------------------------------------------------------------\n",
      "0  | user_emb      | Embedding        | 553 K  | train\n",
      "1  | poi_emb       | Embedding        | 2.6 M  | train\n",
      "2  | emb_dropout   | Dropout          | 0      | train\n",
      "3  | net           | LSTM             | 8.4 M  | train\n",
      "4  | out_projector | Linear           | 5.3 M  | train\n",
      "5  | loss_fn       | CrossEntropyLoss | 0      | train\n",
      "6  | acc1          | AccuracyK        | 0      | train\n",
      "7  | acc5          | AccuracyK        | 0      | train\n",
      "8  | acc10         | AccuracyK        | 0      | train\n",
      "9  | acc20         | AccuracyK        | 0      | train\n",
      "10 | mrr           | MRR              | 0      | train\n",
      "------------------------------------------------------------\n",
      "16.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "16.8 M    Total params\n",
      "67.336    Total estimated model params size (MB)\n",
      "11        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/mosix11/Projects/DL/dlpenv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (9) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "Epoch 49: 100%|█| 9/9 [00:00<00:00, 19.35it/s, v_num=7, Train/Loss_step=3.310, T\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|██▏                 | 1/9 [00:00<00:00, 15.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|████▍               | 2/9 [00:00<00:00, 10.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████▋             | 3/9 [00:03<00:07,  0.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|████████▉           | 4/9 [00:03<00:04,  1.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|███████████         | 5/9 [00:03<00:02,  1.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|█████████████▎      | 6/9 [00:03<00:01,  1.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|███████████████▌    | 7/9 [00:03<00:01,  1.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|█████████████████▊  | 8/9 [00:03<00:00,  2.05it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████████| 9/9 [00:03<00:00,  2.29it/s]\u001b[A\n",
      "Epoch 99: 100%|█| 9/9 [00:00<00:00, 18.96it/s, v_num=7, Train/Loss_step=3.050, T\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|██▏                 | 1/9 [00:00<00:00, 15.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|████▍               | 2/9 [00:00<00:00,  7.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████▋             | 3/9 [00:00<00:01,  5.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|████████▉           | 4/9 [00:00<00:00,  6.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|███████████         | 5/9 [00:00<00:00,  7.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|█████████████▎      | 6/9 [00:00<00:00,  8.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|███████████████▌    | 7/9 [00:00<00:00,  8.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|█████████████████▊  | 8/9 [00:00<00:00,  9.31it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████████| 9/9 [00:00<00:00, 10.24it/s]\u001b[A\n",
      "Epoch 149: 100%|█| 9/9 [00:00<00:00, 17.43it/s, v_num=7, Train/Loss_step=2.680, \u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|██▏                 | 1/9 [00:00<00:00, 15.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|████▍               | 2/9 [00:00<00:00, 10.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████▋             | 3/9 [00:00<00:00,  6.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|████████▉           | 4/9 [00:00<00:00,  7.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|███████████         | 5/9 [00:00<00:00,  7.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|█████████████▎      | 6/9 [00:00<00:00,  8.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|███████████████▌    | 7/9 [00:00<00:00,  9.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|█████████████████▊  | 8/9 [00:00<00:00,  9.90it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████████| 9/9 [00:00<00:00, 10.87it/s]\u001b[A\n",
      "Epoch 199: 100%|█| 9/9 [00:00<00:00, 20.50it/s, v_num=7, Train/Loss_step=2.430, \u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|██▏                 | 1/9 [00:00<00:00, 15.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|████▍               | 2/9 [00:00<00:00,  9.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████▋             | 3/9 [00:00<00:01,  5.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|████████▉           | 4/9 [00:00<00:00,  6.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|███████████         | 5/9 [00:00<00:00,  7.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|█████████████▎      | 6/9 [00:00<00:00,  8.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|███████████████▌    | 7/9 [00:00<00:00,  8.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|█████████████████▊  | 8/9 [00:00<00:00,  9.46it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████████| 9/9 [00:00<00:00, 10.39it/s]\u001b[A\n",
      "Epoch 249: 100%|█| 9/9 [00:00<00:00, 18.57it/s, v_num=7, Train/Loss_step=2.230, \u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|██▏                 | 1/9 [00:00<00:00, 14.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|████▍               | 2/9 [00:00<00:00,  9.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████▋             | 3/9 [00:00<00:01,  5.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|████████▉           | 4/9 [00:00<00:00,  6.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|███████████         | 5/9 [00:00<00:00,  7.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|█████████████▎      | 6/9 [00:00<00:00,  8.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|███████████████▌    | 7/9 [00:00<00:00,  9.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|█████████████████▊  | 8/9 [00:00<00:00,  9.47it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████████| 9/9 [00:00<00:00, 10.31it/s]\u001b[A\n",
      "Epoch 263:  67%|▋| 6/9 [00:00<00:00, 14.79it/s, v_num=7, Train/Loss_step=2.250, ^CA\n",
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    }
   ],
   "source": [
    "!python train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded!\n",
      "Dataset already extracted!\n",
      "Dateset statistics before filtering:\n",
      "Number of records: 227178\n",
      "Number of users: 1083, with min = 100, max = 2693, and avg: 209.76731301939057\n",
      "Number of venues: 38333, with min = 1, max = 1145, and avg: 5.926434142905591\n",
      "Number of venue categories: 400\n",
      "Dateset statistics after filtering:\n",
      "Number of records: 147675\n",
      "Number of users: 1081, with min = 20, max = 2511, and avg: 136.6096207215541\n",
      "Number of venues: 5128, with min = 10, max = 1145, and avg: 28.797776911076443\n",
      "Number of venue categories: 320\n",
      "Spatial graph sparsity: 99.3473171064128\n",
      "Temporal graph sparsity: 91.82665190772997\n"
     ]
    }
   ],
   "source": [
    "from src.dataset import FoursquareNYC\n",
    "from src.models import TrajLSTM\n",
    "\n",
    "ds = FoursquareNYC(batch_size=8, num_workers=1)\n",
    "ds.setup('fit')\n",
    "ds.setup('test')\n",
    "# model = TrajLSTM(num_user=ds.STATS['num_user'],\n",
    "#                  num_pois=ds.STATS['num_pois'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5129, 5129])\n",
      "torch.Size([5129, 5129])\n"
     ]
    }
   ],
   "source": [
    "spatial_graph = ds.spatial_graph\n",
    "temporal_graph = ds.temporal_graph\n",
    "\n",
    "print(spatial_graph.shape)\n",
    "print(temporal_graph.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "dl_train = ds.train_dataloader()\n",
    "dl_test = ds.test_dataloader()\n",
    "batch = next(iter(dl_train))\n",
    "x, y, lens = batch\n",
    "pois = x[1]\n",
    "seq_len = pois.size(1)\n",
    "mask = torch.arange(seq_len).expand(\n",
    "            len(lens), seq_len\n",
    "        ) < lens.unsqueeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TrajLSTM(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([63, 63, 53, 63, 63, 63, 52, 63])\n",
      "tensor([3069,  989,  595, 4240, 4240, 4240, 4240,  989, 4240,  989,  989, 4240,\n",
      "        4240, 4240, 4241, 4240, 4240,  595, 4240, 4241, 4240, 4240, 4241, 4240,\n",
      "         595, 4240, 4241, 4240,  989, 4241, 4240,  989,  224,  339,   38, 4241,\n",
      "        4240, 4241, 4240, 4240, 4241, 4240,  989, 4240, 4240, 4241,  195,  989,\n",
      "        4240, 4241,    9, 4241,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0])\n",
      "tensor([ 989,  595, 4240, 4240, 4240, 4240,  989, 4240,  989,  989, 4240, 4240,\n",
      "        4240, 4241, 4240, 4240,  595, 4240, 4241, 4240, 4240, 4241, 4240,  595,\n",
      "        4240, 4241, 4240,  989, 4241, 4240,  989,  224,  339,   38, 4241, 4240,\n",
      "        4241, 4240, 4240, 4241, 4240,  989, 4240, 4240, 4241,  195,  989, 4240,\n",
      "        4241,    9, 4241, 4240,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0])\n",
      "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True, False, False, False, False, False, False, False, False,\n",
      "        False, False, False])\n"
     ]
    }
   ],
   "source": [
    "print(lens)\n",
    "\n",
    "print(pois[-2])\n",
    "print(y[1][-2])\n",
    "print(mask[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor([3827, 3828, 3829, 4240, 4241, 4934, 5050])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "neighbors = spatial_graph[pois]  # shape: (batch_size, seq_len, num_nodes)\n",
    "masked_neighbors = neighbors * mask.unsqueeze(-1)\n",
    "print(torch.equal(neighbors, masked_neighbors))\n",
    "nonzero_indices = torch.nonzero(spatial_graph[4241], as_tuple=True)[0]\n",
    "print(nonzero_indices)\n",
    "nonzero_indices = torch.nonzero(neighbors[-2][-1], as_tuple=True)[0]\n",
    "print(nonzero_indices)\n",
    "print(torch.sum(spatial_graph[0, :]))\n",
    "# print(neighbors.shape)\n",
    "# print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "torch.Size([7, 1])\n",
      "tensor([3827, 3828, 3829, 4240, 4241, 4934, 5050])\n"
     ]
    }
   ],
   "source": [
    "neighbors = spatial_graph[pois]  # shape: (batch_size, seq_len, num_nodes)\n",
    "masked_neighbors = neighbors * mask.unsqueeze(-1)\n",
    "print(torch.equal(neighbors, masked_neighbors))\n",
    "nonzero_indices = torch.nonzero(spatial_graph[4241])[0]\n",
    "print(nonzero_indices)\n",
    "nonzero_indices = torch.nonzero(masked_neighbors[-2][-12])[0]\n",
    "print(nonzero_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_user': 1081,\n",
       " 'num_pois': 5128,\n",
       " 'num_poi_cat': 320,\n",
       " 'num_time_slots': 56,\n",
       " 'num_gh_P5': 114,\n",
       " 'num_gh_P6': 1120,\n",
       " 'num_gh_P7': 3087}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.STATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1120\n"
     ]
    }
   ],
   "source": [
    "s = 0\n",
    "mdict = ds.hierarchical_spatial_graph['P5_to_P6']\n",
    "\n",
    "for key, value in mdict.items():\n",
    "    s += len(value)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import torch\n",
    "from IPython.display import display\n",
    "widget = widgets.Output()\n",
    "# with widget:\n",
    "#     display(ds.poi_trajectories)\n",
    "# print(ds.user_train_trajectories.iloc[400])\n",
    "ds.setup(stage='fit')\n",
    "ds.setup(stage='test')\n",
    "dl_train = ds.train_dataloader()\n",
    "dl_test = ds.test_dataloader()\n",
    "x, y, lens = next(iter(dl_train)) \n",
    "user_ids = x[0]\n",
    "pois = x[1]\n",
    "seq_len = pois.size(1)\n",
    "mask = torch.arange(seq_len).expand(len(lens), seq_len) < lens.unsqueeze(1)\n",
    "expanded_user_ids = user_ids.unsqueeze(1).repeat(1, pois.size(1))\n",
    "print(lens)\n",
    "print(expanded_user_ids.shape)\n",
    "print(pois.shape)\n",
    "# print(expanded_user_ids.shape)\n",
    "print(expanded_user_ids * mask)\n",
    "# print(pois.shape)\n",
    "# print(y[1][0])\n",
    "# print(y[1].reshape(-1))\n",
    "# print(lens)\n",
    "# print(x[1].shape)\n",
    "# print(next(iter(dl_test)))\n",
    "# for batch in dl_test:\n",
    "#     pass\n",
    "\n",
    "# display(widget)\n",
    "\n",
    "# lengths1 = np.array(ds.user_train_trajectories['Time Slot'].apply(len))\n",
    "# lengths2 = np.array(ds.user_train_trajectories['Venue ID'].apply(len))\n",
    "# lengths3 = np.array(ds.user_train_trajectories['Geohash ID'].apply(len))\n",
    "\n",
    "# ds.plot_distribution(lengths)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
